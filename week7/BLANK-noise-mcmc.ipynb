{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fa327b",
   "metadata": {},
   "source": [
    "## BLANK - noise & mcmc\n",
    "\n",
    "noise is part of working with data so let's learn how it works\n",
    "\n",
    "we will cover\n",
    "- what is noise\n",
    "- nature of noise in data\n",
    "- simulate noisy data\n",
    "- what is mcmc\n",
    "- mcmc on simulated data\n",
    "\n",
    "### what is noise\n",
    "\n",
    "noise in science is a very **__specific__** term (aka johnson nyquist noise) - you can't just say anything is noise. noise comes from the random motion of electrons and is directly proportional to temperature. noise originates in the electronics that are taking data - it does not exist in the source.\n",
    "\n",
    "### noise in data\n",
    "\n",
    "noise adds power to the data and is **gaussian**\n",
    "\n",
    "### simulation - noise\n",
    "\n",
    "imagine there is a source you are observing that emits energy in a gaussian distribution\n",
    "\n",
    "#### imports\n",
    "\n",
    "`pip install` the following if you don't already have them: `numpy`, `matplotlib`, `pymc`, `arviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f15edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the functions we need\n",
    "def gaussian(x, mu, std):\n",
    "    '''\n",
    "    evaluates the values of a gaussian distrubition given data\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        x data to evaluate a gaussian on\n",
    "    mu : float\n",
    "        mean of the gaussian\n",
    "    std : float\n",
    "        standard deviation of the gaussian\n",
    "\n",
    "    returns\n",
    "    ----------\n",
    "    array-like : evaluated distribution with the same shape as x\n",
    "    '''\n",
    "\n",
    "    return ___\n",
    "\n",
    "# simulate a gaussian source\n",
    "mu = 500    # mean\n",
    "sigma = 200  # standard deviation\n",
    "n = 500     # number of data points\n",
    "\n",
    "x = np.linspace(0, 1000, n)\n",
    "data = ___\n",
    "\n",
    "# add noise to the data\n",
    "noise = np.random.___(n) # noise from a standard normal distribution\n",
    "data = data + ___\n",
    "\n",
    "plt.plot(x, data)   # histogram to see the data\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d21514",
   "metadata": {},
   "source": [
    "### what is mcmc\n",
    "\n",
    "mcmc stands for markov chain monte carlo. please look into what markov chains are and what monte carlo means. this is just a way for you to see how this works. mcmc is a machine learning algorithm that allows you to fit paramters to a distribution. in other words, you can give it a function and some data and it will estimate the arguments into the function that would produce that data. it chooses the parameters that minimize the least-squares difference between the distribution and the data (aka minimizing the loss).\n",
    "\n",
    "how does it do this? imagine a hilly terrain where walking in one dimension is changing one parameter (say, x) and the other direction is another parameter (say, y). we want to figure out which combination of x and y represents our data. we want to find f(x, y) that produced the data. this hilly terrain is known as the parameter space and the correct combination of x and y is on top of the highest hill. what does the f(x, y) represent? it is the loss function but negated so that the minimum loss is at the top of the highest hill.\n",
    "\n",
    "we want to start somewhere random and make tiny random changes to x and y to see which changes are effective in decreasing the loss with respect to where we are currently standing. this keeps going until we reach the top of the highest hill. we know we are there because any change we make would cause the f(x, y) of the new spot to be less than the one we currenly stand on.\n",
    "\n",
    "the implementation we will use does something clever here. it will randomly take that step downhill. why? in case we are stuck on a local maxima - a hill that isn't the tallest hill. it also has a No U-Turns (NUTS) sampler that does not retrace steps to save time and energy.\n",
    "\n",
    "this was a LOT of ML jargon but it's helpful to know how your algorithms work. this is a very surface level view of mcmc so please read more online if you're interested, it's super cool.\n",
    "\n",
    "### simulation - mcmc\n",
    "\n",
    "let's put the simulated data through mcmc to see if we can pull the original gaussian parameters back out!\n",
    "again lots of ML jargon so please ask questions and look up more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2670ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc implementation\n",
    "import pymc as pm\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb87544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mcmc trial\n",
    "def mcmc_trial(x, data):\n",
    "    # start a 'model context' - this defines a space for all the vars and likelihoods for this model\n",
    "    with pm.Model() as model:\n",
    "        # define a 'prior distribution' for the mean (can start anywhere btwn 0 and 1000 w equal probability)\n",
    "        mu = pm.___('mu', lower=___, upper=___)  \n",
    "\n",
    "        # define a prior over the standard deviation (lognormal since stdvs are always > 0)\n",
    "        sigma = pm.___('sigma', mu=np.log(100), sigma=1)\n",
    "\n",
    "        # feed the gaussian into the model\n",
    "        model_prediction = ___\n",
    "\n",
    "        # likelihood function - gives the model a way to work with predictions vs observations\n",
    "        pm.Normal(\"obs\", mu=model_prediction, sigma=1.0, observed=data)\n",
    "\n",
    "        # sample from posterior distribution\n",
    "        trace = pm.sample(draws=1000, tune=1000, target_accept=0.9, return_inferencedata=True)\n",
    "        return trace\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run mcmc\n",
    "\n",
    "run = mcmc_trial(___, ___)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the mcmc results as a pandas dataframe\n",
    "az.summary(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to actual values\n",
    "mu,sigma\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
